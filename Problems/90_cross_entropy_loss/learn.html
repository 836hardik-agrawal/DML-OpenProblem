<h2>Multi-class Cross-Entropy Loss Implementation</h2>

<p>
  Cross-entropy loss, also known as log loss, measures the performance of a
  classification model whose output is a probability value between 0 and 1. For
  multi-class classification tasks, we use the categorical cross-entropy loss.
</p>

<h3>Mathematical Background</h3>

<p>
  For a single sample with C classes, the categorical cross-entropy loss is
  defined as:
</p>

\[ L = -\sum_{c=1}^{C} y_c \log(p_c) \]

<p>where:</p>
<ul>
  <li>
    \(y_c\) is a binary indicator (0 or 1) if class label c is the correct
    classification for the sample
  </li>
  <li>
    \(p_c\) is the predicted probability that the sample belongs to class c
  </li>
  <li>\(C\) is the number of classes</li>
</ul>

<h3>Implementation Requirements</h3>

<p>
  Your task is to implement a function that computes the average cross-entropy
  loss across multiple samples:
</p>

\[ L_{batch} = -\frac{1}{N}\sum_{n=1}^{N}\sum_{c=1}^{C} y_{n,c} \log(p_{n,c}) \]

<p>where N is the number of samples in the batch.</p>

<h3>Important Considerations</h3>

<ul>
  <li>Handle numerical stability by adding a small epsilon to avoid log(0)</li>
  <li>Ensure predicted probabilities sum to 1 for each sample</li>
  <li>Return average loss across all samples</li>
  <li>Handle invalid inputs appropriately</li>
</ul>

<p>
  The function should take predicted probabilities and true labels as input and
  return the average cross-entropy loss.
</p>
